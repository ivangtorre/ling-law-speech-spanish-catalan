{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy.stats\n",
    "import more_itertools as mit\n",
    "import urllib\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograma(dataset, numero_bins=50, type_bins='log'):\n",
    "    \"\"\"\n",
    "    Calcula el histograma de una serie de datos y guarda el resultado\n",
    "    El input es una lista, no sirve meter como input un histograma X-Y\n",
    "    INPUT:\n",
    "        dataset (list, array): Datos sobre los que se hacen el histograma\n",
    "        header (str): String for using in header and in file name\n",
    "        output_path (str): Donde se guardara el resultado. IF OUTPUT_PATH=FALSE\n",
    "                            devuelve el histograma y no lo guarda a archivo\n",
    "        numero_bins (int): Numero de bins que se realizaran\n",
    "        type_bins ('log', 'lin'): Relacion para generar los bins\n",
    "        output_name (str): Name for the output file with extension .txt\n",
    "        header_adicional (str): Aditional header info only for the file header\n",
    "    OUTPUT:\n",
    "        Guarda el resultado en output_path\n",
    "        if output_path == FALSE: Devuelve bin_mean_x, counts_mean\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = np.asarray(dataset)\n",
    "\n",
    "    # Defining edges depending if log or lin\n",
    "    if (type_bins == 'log'):\n",
    "        min_x = np.log10(np.min(dataset[dataset.nonzero()]))\n",
    "        max_x = np.log10(np.max(dataset))\n",
    "        bin_edges = np.logspace(min_x, max_x, numero_bins)\n",
    "        \n",
    "    elif (type_bins == 'lin'):\n",
    "        min_x = np.min(dataset[dataset.nonzero()])\n",
    "        max_x = np.max(dataset)\n",
    "        bin_edges = np.linspace(min_x, max_x, numero_bins)\n",
    "\n",
    "    # En que bin cae cada dato\n",
    "    index = np.digitize(dataset, bin_edges)\n",
    "\n",
    "    # Frecuencia de datos por bin\n",
    "    datosPorBin = [len(dataset[index == i]) for i in range(1, len(bin_edges))]\n",
    "\n",
    "    # Adimensionaliza la frecuencia por la anchura del bin\n",
    "    counts_mean = datosPorBin/np.diff(bin_edges)\n",
    "    counts_mean = counts_mean[counts_mean != 0]\n",
    "    \n",
    "    # version normalizada\n",
    "    masatotal = np.sum(datosPorBin)\n",
    "    counts_norm = counts_mean/masatotal\n",
    "\n",
    "    # Posicion media del eje x\n",
    "    bin_mean_x = [dataset[index == i].mean() for i in range(1, len(bin_edges))\n",
    "                  if len(np.asarray(dataset[index == i])) > 0]\n",
    "\n",
    "    return(bin_mean_x, counts_mean, counts_norm, bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binear_datos(lista_x, lista_y, bins=50, log = True):\n",
    "    \"\"\"\n",
    "    Dado una lista de par de puntos (X,Y) binea dando lugar a una lista equivalente\n",
    "    pero de menor dimension. Sirve para eliminar el ruido, variaciones, etc.\n",
    "    INPUT:\n",
    "        lista_x (list, array): Lista de puntos de igual dimension que lista_y\n",
    "        lista_y (list, array): Lista de puntos de igual dimension que lista_x\n",
    "        bins (int): numero de bins a utilizar\n",
    "        log (boolean): True-> bins logaritmico, False-> bins lineales\n",
    "    OUTPUT:\n",
    "        posiciones (array): Centro de gravedad del bin. lista de posiciones X\n",
    "        frecuencia_media(array): Frecuencia pesada en los bins\n",
    "    \"\"\"\n",
    "    # Definicion de los bins\n",
    "    if log:\n",
    "        bin_edges = np.logspace(np.log10(min(lista_x)), np.log10(max(lista_x)), bins)\n",
    "    else:\n",
    "         bin_edges = np.linspace(min(lista_x), max(lista_x), bins)\n",
    "\n",
    "    diccionario = dict() # valor*posicion\n",
    "    diccionario_frecs = dict()\n",
    "    for indice_x, value_x in enumerate(lista_x):\n",
    "        indice_caja = np.searchsorted(bin_edges, value_x)\n",
    "        if indice_caja in diccionario:\n",
    "            diccionario[indice_caja].append(lista_x[indice_x]*lista_y[indice_x])\n",
    "            diccionario_frecs[indice_caja].append(lista_y[indice_x])\n",
    "        else:\n",
    "            diccionario[indice_caja] = list()\n",
    "            diccionario_frecs[indice_caja] = list()\n",
    "            diccionario[indice_caja].append(lista_x[indice_x]*lista_y[indice_x])\n",
    "            diccionario_frecs[indice_caja].append(lista_y[indice_x])\n",
    "\n",
    "    frecuencia_media = []\n",
    "    for key, value in diccionario_frecs.items():\n",
    "        if len(value) > 0:\n",
    "            frecuencia_media.append(np.mean(value))\n",
    "\n",
    "        elif len(value) == 0:\n",
    "            frecuencia_media.append(0)\n",
    "\n",
    "    posiciones = []\n",
    "    for key, value in diccionario.items():\n",
    "        if len(value) > 0:\n",
    "            posiciones.append(np.sum(value)/np.sum(diccionario_frecs[key]))\n",
    "        elif len(value) == 0:\n",
    "            posiciones.append((bin_edges[key] + bin_edges[key+1])/2)\n",
    "            \n",
    "    ## Ordenamos los datos\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\"pos\": posiciones, \"freqs\": frecuencia_media})\n",
    "    df = df.sort_values(\"pos\")\n",
    "\n",
    "    posiciones = df.pos.values\n",
    "    frecuencia_media = df.freqs.values\n",
    "    \n",
    "    return posiciones, frecuencia_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subplot_axes(ax, rect, fig, axisbg='w'):\n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    inax_position  = ax.transAxes.transform(rect[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= rect[2]\n",
    "    height *= rect[3]  # <= Typo was here\n",
    "    subax = fig.add_axes([x,y,width,height],axisbg=axisbg, xscale = 'log', yscale ='log')\n",
    "    #subax.xaxis.tick_top()\n",
    "    subax.xaxis.set_label_position('bottom') \n",
    "    #subax.yaxis.tick_right()\n",
    "    subax.yaxis.set_label_position('left')\n",
    "    x_labelsize = subax.get_xticklabels()[0].get_size()\n",
    "    y_labelsize = subax.get_yticklabels()[0].get_size()\n",
    "    x_labelsize *= rect[2]**0.5\n",
    "    y_labelsize *= rect[3]**0.5\n",
    "    subax.xaxis.set_tick_params(labelsize=x_labelsize)\n",
    "    subax.yaxis.set_tick_params(labelsize=y_labelsize)\n",
    "    return(subax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment for debugging\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic laws in speech: the case of Catalan and Spanish\n",
    "REQUISITO: Tener una tabla con las duraciones de cada fonema, palabra, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"TablaTranscripcionGlissandoCat.csv\"\n",
    "output_path = \"\"\n",
    "\n",
    "# Cargar datos\n",
    "tabla_datos = pd.read_csv(input_path, delimiter=\",\", skiprows=1, \n",
    "                            names=['token', 'tinit', 'tend', 'duration', 'sentence', 'path', 'tipe', 'numtoken', 'numphonemes','numletters'])\n",
    "# Para recuperar los okeys\n",
    "#tabla_datos[tabla_datos.numtoken.isin(tabla_datos[tabla_datos.token == \"okay\"].numtoken)]\n",
    "tabla_datos.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos y nos quedamos solo con palabras\n",
    "# No utilizamos News porque repiten el mismo dialogo\n",
    "lista_words = tabla_datos[((tabla_datos.tipe == 'w') & (~tabla_datos.path.str.contains(\"News\"))\n",
    "                           & (tabla_datos.token != 'pause') & (tabla_datos.token != \"CPsil\")\n",
    "                           & (tabla_datos.token != \"CPRCsp\"))]\n",
    "\n",
    "#lista_words.path.str.contains(\"Turns\"))\n",
    "\n",
    "# Creamos una columna que indique el numero de caracteres de esa palabra\n",
    "lista_words = lista_words.assign(dur_characteres=lista_words.token.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos y nos quedamos solo con fonemas\n",
    "lista_fonema = tabla_datos[((tabla_datos.tipe == 'p') & (~tabla_datos.path.str.contains(\"News\"))) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de fonemas \n",
    "numero= lista_fonema.groupby(\"token\").count().tinit \n",
    "len(numero[numero>40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos las duraciones de las frases\n",
    "dur_sentences = tabla_datos[tabla_datos.tipe == \"p\"].groupby(\"sentence\").sum().duration\n",
    "# print(lista_fonema.token.unique())\n",
    "# print(len(lista_fonema.token.unique()))\n",
    "# tabla_datos[tabla_datos.token == \"the\"].head()\n",
    "# tabla_datos[tabla_datos.numtoken == 131]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener las tablas 1 y 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "def imprimir_estadistica(listaduration, rnd, string):\n",
    "    \n",
    "    phonN = len(listaduration)\n",
    "    phonMean = np.round(np.mean(listaduration), rnd)\n",
    "    phonStd = np.round(np.std(listaduration), rnd)\n",
    "    phonMode = mode(np.round(listaduration, rnd))\n",
    "    phonMedian = np.round(np.median(listaduration), rnd)\n",
    "    phonP10 = np.round(np.percentile(listaduration, 10), rnd)\n",
    "    phonP90 = np.round(np.percentile(listaduration, 90), rnd)\n",
    "    file.write(string + str(phonN) +\"  \"+ str(phonMean) + \"  \" + str(phonStd) +\n",
    "      \"  \" + str(phonMode) + \"  \" + str(phonMedian) + \"  \" + str(phonP10) +\n",
    "     \"  \" + str(phonP90))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "\n",
    "file = open(output_path + \"0_Time_durations.txt\",\"w\") \n",
    "file.write(\"                      Time Duration (s)\\n\")\n",
    "file.write(\"                n     Mean  Std  Mode  Median  p10  p90\\n\")\n",
    "\n",
    "imprimir_estadistica(lista_fonema.duration, 2, \"Phoneme:   \")\n",
    "imprimir_estadistica(lista_words.duration, 2, \"Word:      \")\n",
    "imprimir_estadistica(dur_sentences, 1, \"BG:          \")\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_estadistica(listaduration, rnd, string):\n",
    "    \n",
    "    phonMean = np.round(np.mean(listaduration), rnd)\n",
    "    phonStd = np.round(np.std(listaduration), rnd)\n",
    "    phonMode = mode(np.round(listaduration, rnd))\n",
    "    phonMedian = np.round(np.median(listaduration), rnd)\n",
    "    phonP10 = np.round(np.percentile(listaduration, 10), rnd)\n",
    "    phonP90 = np.round(np.percentile(listaduration, 90), rnd)\n",
    "    file.write(string + str(phonMean) + \"  \" + str(phonStd) +\n",
    "      \"  \" + str(phonMode) + \"  \" + str(phonMedian) + \"  \" + str(phonP10) +\n",
    "     \"  \" + str(phonP90))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "file = open(output_path + \"0_chars_phons_words_durations.txt\",\"w\") \n",
    "file.write(\"                  Num Characters\\n\")\n",
    "file.write(\"             n     Mean  Std  Mode  Median  p10  p90\\n\")    \n",
    "\n",
    "numcharperphoneme = lista_words.numletters/lista_words.numphonemes\n",
    "numcharperphoneme = numcharperphoneme[numcharperphoneme<7]\n",
    "\n",
    "imprimir_estadistica(numcharperphoneme, 1, \"Phoneme:   \")\n",
    "imprimir_estadistica(lista_words.numletters, 0, \"Word:      \")\n",
    "char_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().numletters\n",
    "imprimir_estadistica(char_sentences, 0, \"BG:       \")\n",
    "\n",
    "file.write(\"\\n\")\n",
    "file.write(\"                    Num Phonemas\\n\")\n",
    "imprimir_estadistica(lista_words.numphonemes, 1, \"Word:      \")\n",
    "phon_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().numphonemes\n",
    "imprimir_estadistica(phon_sentences, 0, \"BG:      \")\n",
    "\n",
    "file.write(\"\\n\")\n",
    "file.write(\"                     Num Words\\n\")\n",
    "words_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").count().numphonemes\n",
    "imprimir_estadistica(words_sentences, 0, \"BG:      \")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definimos adecuadamente el subconjunto de fonemas y palabras\n",
    "fonemes_durationsall = lista_fonema.duration.values[(lista_fonema.duration.values<2) & (lista_fonema.duration.values>=0.003)]\n",
    "words_durationsall = lista_words.duration.values[(lista_words.duration.values<5) & (lista_words.duration.values>=0.003) ]\n",
    "dur_sentencesall = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().duration\n",
    "\n",
    "listainformant = [\"s01/\", \"s02/\", \"s03/\", \"s04/\", \"s05/\", \"s06/\", \"s07/\", \"s08/\", \"s09/\"]\n",
    "\n",
    "for i in range(1):\n",
    "    if i == 0:\n",
    "        fonemes_durations = fonemes_durationsall.round(2)\n",
    "        words_durations = words_durationsall\n",
    "        dur_sentences = dur_sentencesall\n",
    "\n",
    "    else:\n",
    "        lista_fonema2 = lista_fonema[lista_fonema.path.str.startswith(listainformant[i-1])]\n",
    "        fonemes_durations2 = lista_fonema2.duration.values[(lista_fonema2.duration.values<5) & (lista_fonema2.duration.values>0)]\n",
    "\n",
    "        lista_words2 = lista_words[lista_words.path.str.startswith(listainformant[i-1])]\n",
    "        words_durations2 = lista_words2.duration.values[(lista_words2.duration.values<5) & (lista_words2.duration.values>0) ]\n",
    "\n",
    "        tabla_datos2 = tabla_datos[tabla_datos.path.str.startswith(listainformant[i-1])]\n",
    "        dur_sentences2 = tabla_datos2[tabla_datos2.tipe == \"w\"].groupby(\"sentence\").sum().duration\n",
    "                       \n",
    "        fonemes_durations = fonemes_durations2\n",
    "        words_durations = words_durations2\n",
    "        #dur_sentences = dur_sentences2\n",
    "\n",
    "\n",
    "    # Realizamos los histogramas\n",
    "#     import collections\n",
    "#     c = collections.Counter(fonemes_durations)\n",
    "#     c = dict(sorted(c.items()))\n",
    "#     bin_mean_x_fon = np.asarray(list(c.keys()))\n",
    "#     counts_norm_fon = np.asarray(list(c.values()))\n",
    "#     masa_total = (counts_norm_fon*0.01).sum()\n",
    "#     counts_norm_fon=counts_norm_fon/masa_total\n",
    "    \n",
    "    \n",
    "    bin_mean_x_fon, _, counts_norm_fon, _ = histograma(fonemes_durations, numero_bins=10)\n",
    "    bin_mean_x_word, _, counts_norm_word, _ = histograma(words_durations, numero_bins=13)\n",
    "    bin_mean_x_sentence, _, counts_norm_sentence, _ = histograma(dur_sentences, numero_bins=12)\n",
    "\n",
    "\n",
    "    # Ploteamos\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(bin_mean_x_fon, counts_norm_fon, '-s', lw = 2, ms = 7, label= 'phonemes', zorder=1, c=\"darkorange\")\n",
    "    ax.plot(bin_mean_x_word, counts_norm_word, '-o', lw = 2, ms = 7, label= 'words', zorder=2, c='darkblue')\n",
    "    ax.plot(bin_mean_x_sentence, counts_norm_sentence, '-d', lw = 2, ms = 7, label= 'BG', zorder=3, c=\"g\")\n",
    "\n",
    "\n",
    "    #ax.set_ylim([-0.3, 11.3])\n",
    "    ax.set_xlim([0, 1.5])\n",
    "\n",
    "    ax.set_xlabel(\"t (seconds)\", fontsize=14)\n",
    "    ax.set_ylabel(\"P(t)\", fontsize=14)\n",
    "\n",
    "    # Table\n",
    "\n",
    "\n",
    "    col_labels=[r'$<t>$',r'$mode$']\n",
    "    row_labels=['phon','words','senten']\n",
    "    table_vals=[[np.round(np.mean(lista_fonema.duration.values[lista_fonema.duration.values<4]),3),\n",
    "                 scipy.stats.mode(np.round(lista_fonema.duration.values[lista_fonema.duration.values<4], 2))[0][0]],\n",
    "\n",
    "                [np.round(np.mean(lista_words.duration.values[lista_words.duration.values<4]),2),\n",
    "                 scipy.stats.mode(np.round(lista_words.duration.values[lista_words.duration.values<4], 2))[0][0]],\n",
    "\n",
    "               [np.round(np.mean(dur_sentences), 1),\n",
    "                   scipy.stats.mode(np.round(dur_sentences, 2))[0][0]]]\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ## COLAPSO DE LOS DATOS##############3\n",
    "    # Vamos a intentar colapsar los datos\n",
    "    log_fonemes_dur = np.log(fonemes_durations)\n",
    "    log_fonemes_dur = (log_fonemes_dur - np.mean(log_fonemes_dur))/np.std(log_fonemes_dur)\n",
    "\n",
    "    log_words_dur = np.log(words_durations)\n",
    "    log_words_dur = (log_words_dur - np.mean(log_words_dur))/np.std(log_words_dur)\n",
    "\n",
    "    log_sentences_dur = np.log(dur_sentences)\n",
    "    log_sentences_dur = (log_sentences_dur - np.mean(log_sentences_dur))/np.std(log_sentences_dur)\n",
    "\n",
    "    # Datos Normales 0,1\n",
    "    randomNormal = np.random.normal(0, 1, 10000000)\n",
    "\n",
    "    # Realizamos los histogramas\n",
    "    bin_mean_x_fon_collapsed, _, counts_norm_fon_collapsed, _ = histograma(log_fonemes_dur, numero_bins=10, type_bins=\"lin\")\n",
    "    bin_mean_x_word_collapsed, _, counts_norm_word_collapsed, _ = histograma(log_words_dur, numero_bins=14, type_bins=\"lin\")\n",
    "    bin_mean_x_sentence_collapsed, _, counts_norm_sentence_collapsed, _ = histograma(log_sentences_dur, numero_bins=10, type_bins=\"lin\")\n",
    "\n",
    "    bin_mean_x_normal, _, counts_norm_normal, _ = histograma(randomNormal, numero_bins=200, type_bins=\"lin\")\n",
    "\n",
    "\n",
    "    # Ploteamos\n",
    "    subax = plt.axes([0.55, 0.53, .35, .35])\n",
    "    subax.plot(bin_mean_x_fon_collapsed, counts_norm_fon_collapsed, 's', lw = 2, ms = 7, label= 'phonemes', zorder=1, c=\"darkorange\")\n",
    "    subax.plot(bin_mean_x_word_collapsed, counts_norm_word_collapsed, 'o', lw = 2, ms = 7, label= 'words', zorder=2, c='darkblue')\n",
    "    subax.plot(bin_mean_x_sentence_collapsed, counts_norm_sentence_collapsed, 'd', lw = 2, ms = 7, label= 'BG', zorder=3, c=\"g\")\n",
    "    subax.plot(bin_mean_x_normal, counts_norm_normal, '-s', lw = 1, ms = 0, label= 'phonemes', zorder=5, c = 'k')\n",
    "\n",
    "\n",
    "    subax.set_xlim([-5.1,5.1])\n",
    "    #subax.set_ylim([0, 0.53])\n",
    "    subax.set_xticks([-3,0,3])\n",
    "    subax.set_yticks([0, 0.2, 0.4])\n",
    "\n",
    "    subax.set_xlabel(\"t'\", fontsize=14)\n",
    "\n",
    "\n",
    "    subax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    ax.set_xticks([0, 0.5, 1, 1.5])\n",
    "    #ax.set_yticks([0, 8, 16])\n",
    "\n",
    "    ax.legend(loc=(0.11, 0.61), frameon=False, fontsize = 12, title=r\"$Catalan$\", title_fontsize=12)\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        f.savefig(output_path + \"1_Probability_distribution_duration.pdf\")\n",
    "#         ax.set_yscale(\"log\")\n",
    "#         ax.set_ylim([0.001, 13])\n",
    "#         ax.set_xlim([0, 5])\n",
    "#         ax.set_xticks([0, 2, 4])\n",
    "\n",
    "#         f.savefig(\"SI_LOG_1_Probability_distribution_duration.pdf\")\n",
    "\n",
    "    else:        \n",
    "        f.savefig(\"1_Probability_distribution_duration_informant\"+str(i)+\".pdf\", bbox_inches='tight')\n",
    "\n",
    "        \n",
    "##########################################\n",
    "# Realizamos los histogramas\n",
    "fonemes_durations = lista_fonema.duration.values[(lista_fonema.duration.values<5) & (lista_fonema.duration.values>=0.03)]\n",
    "words_durations = lista_words.duration.values[(lista_words.duration.values<5) & (lista_words.duration.values>=0.03) ]\n",
    "dur_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().duration\n",
    "\n",
    "bin_mean_x_fon, _, counts_norm_fon, _ = histograma(fonemes_durationsall, numero_bins=35)\n",
    "bin_mean_x_word, _, counts_norm_word, _ = histograma(words_durationsall, numero_bins=15)\n",
    "bin_mean_x_sentence, _, counts_norm_sentence, _ = histograma(dur_sentencesall, numero_bins=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ley de Zipf\n",
    "Calculamos la ley de zip (corpus escrito) para el articulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lista_fonema_token_process = lista_fonema.token.str.split(\";\", expand=True)[0]\n",
    "lista_fonema_token_process = lista_fonema_token_process.str.split(\"+\", expand=True)[0]\n",
    "\n",
    "\n",
    "freqs_words = lista_words.token.value_counts()\n",
    "freqs_phonemes = lista_fonema_token_process.value_counts()\n",
    "\n",
    "freqs_words.index = np.arange(1, len(freqs_words) + 1)\n",
    "freqs_words.to_csv(output_path+ \"zipf_data.csv\")\n",
    "\n",
    "freqs_phonemes.index = np.arange(1, len(freqs_phonemes) + 1)\n",
    "\n",
    "\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(freqs_words[0:30000], 'o', lw = 2, ms = 8, label= 'words', zorder=2, alpha=1, c='darkblue')\n",
    "ax.plot(freqs_phonemes[0:35], 's', lw = 2, ms = 8, label= 'phonemes', zorder=1,alpha=1, c='darkorange')\n",
    "\n",
    "\n",
    "# # PENDIENTE PHONEMAS\n",
    "xphoneme = freqs_phonemes.index.values\n",
    "yplotphon = xphoneme**(-0.03759105) * 0.9061079**xphoneme\n",
    "yplotphon = yplotphon/yplotphon[0]*freqs_phonemes.values[0]\n",
    "ax.plot(xphoneme, yplotphon, \"--\", color=\"k\", lw = 2)\n",
    "ax.text(0.4, 0.93, \"$Yule(0.04, 0.90)$\", horizontalalignment='center', fontsize = 12, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "# WORDS\n",
    "# Line pendiente\n",
    "xline1 = np.array([100, 5000])\n",
    "yline1 = np.power(1.5*10**-4*xline1, -1.42)\n",
    "\n",
    "ax.plot(xline1, yline1 , \"--\", color = \"black\", lw = 2)\n",
    "ax.text(0.8, 0.4, r'$\\alpha = 1.42$', horizontalalignment='center', fontsize = 12, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "ax.set_xlabel(r\"$rank$\",fontsize=14)\n",
    "ax.set_ylabel(r\"$Freq(r)$\", fontsize=14)\n",
    "ax.legend(fontsize=13, frameon=False, loc=3, title=r\"$Catalan$\", title_fontsize=12)\n",
    "ax.set_ylim([0.3, 1*10**5])\n",
    "\n",
    "f.savefig(output_path + \"2_Zipf_law.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Heaps Law\n",
    "Para heap law, leemos secuencialmente las conversaciones para ver como aumenta el vocabulario en funcion del tiempo transcurrido o del numero de palabras transcurridas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def funcionlin(x, A, B): # this is your 'straight line' y=f(x)\n",
    "    return A*x + B\n",
    "##########################33\n",
    "\n",
    "import random\n",
    "random.seed(10)\n",
    "lista_words[\"informant\"] = lista_words.path.str[0:25]\n",
    "\n",
    "lista_pendientes=[]\n",
    "\n",
    "\n",
    "# FIGURA HEAPS RANDOM\n",
    "f, ax = plt.subplots()\n",
    "ax2 = ax.twiny()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    groups = [df for _, df in lista_words.groupby(\"informant\")]\n",
    "    random.shuffle(groups)\n",
    "    lista_word_rand = pd.concat(groups).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Primero añadimos una columna de zeros\n",
    "    lista_word_rand['first_time_token'] = 0 \n",
    "    first_ocurrences_rand =  lista_word_rand.drop_duplicates(subset=[\"token\"]).index\n",
    "    lista_word_rand.loc[first_ocurrences_rand, \"first_time_token\"] = 1\n",
    "\n",
    "\n",
    "    # Creamos el fichero de tiempos\n",
    "    total_time = 0\n",
    "    lista_tiempo_transcurrido = []\n",
    "    path = list(lista_word_rand[0:1].path)[0]\n",
    "\n",
    "    for index, row in lista_word_rand.iterrows():\n",
    "        if path != row.path:\n",
    "            total_time += previoustime # Guarda\n",
    "            path = row.path\n",
    "        previoustime = row.tinit\n",
    "        lista_tiempo_transcurrido.append(total_time+row.tinit)\n",
    "\n",
    "    xheapt_rand = lista_tiempo_transcurrido\n",
    "    yheapt_rand = lista_word_rand.first_time_token.cumsum()\n",
    "\n",
    "    #######SLOPE CALCULATION ##################\n",
    "    popt, pcov = curve_fit(funcionlin, np.log(xheapt_rand[100::]),np.log(yheapt_rand[100::])) # \n",
    "    lista_pendientes.append(popt[0])\n",
    "    #########################################################\n",
    "    \n",
    "    # decimate logaritmic\n",
    "    decimation = np.logspace(0, np.log10(len(xheapt_rand)-1),100, dtype=\"int\")\n",
    "    decimation = np.asarray([0] + list(decimation))\n",
    "\n",
    "    xheapt_rand = np.asarray(xheapt_rand) - xheapt_rand[1] - 0.2\n",
    "    xheaptdecimate_rand = pd.DataFrame(xheapt_rand).loc[decimation, :]\n",
    "    xheaptdecimate_rand = xheaptdecimate_rand[0].values\n",
    "\n",
    "    yheaptdecimate_rand = pd.DataFrame(list(yheapt_rand)).loc[decimation, :]\n",
    "    yheaptdecimate_rand = yheaptdecimate_rand[0].values\n",
    "\n",
    "    xheapL_rand = pd.DataFrame(lista_word_rand.index.values + 1).loc[decimation, :]\n",
    "    xheapL_rand = xheapL_rand[0].values\n",
    "    \n",
    "    # Heaps corpus escrito\n",
    "    yheapL_rand = pd.DataFrame(list(yheapt_rand)).loc[decimation, :]\n",
    "    yheapL_rand = yheapL_rand[0].values\n",
    "    \n",
    "    \n",
    "    # PLOT\n",
    "    if i == 0:\n",
    "        lns1 = ax.plot(2*xheaptdecimate_rand, yheaptdecimate_rand -2 , '-o', lw = 1, ms = 4, label= 'V(T)', zorder=2, alpha=0.6, color=\"darkblue\")\n",
    "        lns2 = ax2.plot(xheapL_rand, yheaptdecimate_rand, '-d', lw = 1, ms = 4, label= 'V(L)', zorder=5, alpha=0.6, color = \"darkolivegreen\")\n",
    "    # PLOT\n",
    "    if i != 0:\n",
    "        lns10 = ax.plot(2*xheaptdecimate_rand, yheaptdecimate_rand -2 , '-o', lw = 1, ms = 4, zorder=2, alpha=0.6, color=\"darkblue\")\n",
    "        lns20 = ax2.plot(xheapL_rand, yheaptdecimate_rand, '-d', lw = 1, ms = 4, zorder=5, alpha=0.6, color = \"darkolivegreen\")\n",
    "    \n",
    "    \n",
    "xline = np.array([2*10**2, 5*10**4])\n",
    "ax.text(0.82, 0.57, r'$\\beta = \\gamma =$' + str(np.mean(lista_pendientes).round(2)), horizontalalignment='center', fontsize = 14, verticalalignment='center', transform=ax.transAxes)\n",
    "ax.plot(xline, 2* np.power(xline, np.mean(lista_pendientes)), \"--\", color = \"k\", lw=2)\n",
    "\n",
    "ax.set_xlim([5*10**-1, 2*10**5])\n",
    "\n",
    "ax.set_xlabel(r\"$Time$\" +\" \" + r\"$elapsed$\" +\" \" + r\"$T$\"+ \" \" + r\"$(seconds)$\", fontsize=14)\n",
    "ax.set_ylabel( r\"$Vocabulary$\" + \" \" +  r\"$V$\", fontsize=14)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_xlim([5*10**-1, 2*10**5])\n",
    "\n",
    "\n",
    "ax2.set_xlabel(r\"$Words$\" +\" \" + r\"$elapsed$\" + \" \" + r\"$L$\", fontsize=14)\n",
    "\n",
    "\n",
    "# added these three lines\n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc=2, frameon = False, fontsize = 14, title=r\"$Catalan$\", title_fontsize=12)\n",
    "\n",
    "\n",
    "#ax.set_xticks([10**0,10**2, 10**4, 10**4])\n",
    "#ax2.set_xticks([10**0,10**2, 10**4, 10**5])\n",
    "#ax.set_yticks([10**0,10**2, 10**4])\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax2.tick_params(axis='x', labelsize=12)\n",
    "\n",
    "\n",
    "f.savefig(output_path + \"4_HeapLaw_RANDOM_order.pdf\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Brevity law \n",
    "Calculamos la ley de brevedad chequeando que las palabras más recuentes tienden a ser más cortas\n",
    "Entendemos tres posibles definiciones:\n",
    "\n",
    "\n",
    "## 4.1 WORDS\n",
    "1. Frequencia palabras vs duracion en segundos\n",
    "2. Frecuencia palabras vs numero de fonemas\n",
    "3. Frecuencia de palabras vs numero de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos una columna que indique las veces que ha ocurrido esa palabra en nuestra base de datos\n",
    "lista_words = lista_words.assign(repetitions=lista_words.token.map(lista_words.token.value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func_exp(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "def func_log(x, a, b): # FUNCION LOGARITMICA que utiliza los mismos parametros\n",
    "    return 1/(-b) * ( np.log(x) - np.log(a) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Frequencia palabras vs duracion segundos\n",
    "#brevity0 = lista_words.groupby(\"repetitions\").duration.mean()\n",
    "brevity0 = lista_words.groupby(\"token\").median()[[\"duration\", \"repetitions\"]]\n",
    "\n",
    "\n",
    "#The two-sided p-value for a hypothesis test whose null hypothesis is that two sets of data are uncorrelated, \n",
    "#A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis\n",
    "print(\"Frequencia palabras vs duracion(segundos:\")\n",
    "print(scipy.stats.spearmanr(brevity0.duration, np.log(brevity0.repetitions)))\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "\n",
    "### AHORA VAMOS A HACER UN BINING\n",
    "freq0, pos0  = binear_datos(brevity0.repetitions, brevity0.duration, bins=11, log = True)\n",
    "\n",
    "ax.plot(brevity0.duration, brevity0.repetitions, 'o', lw = 1, ms = 1, zorder=2, alpha=0.6, color = 'lightgray',fillstyle= \"none\")\n",
    "ax.plot(pos0, freq0, 'o', lw = 2, ms = 10, label= 'words', zorder=2, alpha=1, color=\"darkblue\")\n",
    "\n",
    "ax.set_xlabel(r\"$Median$\"+ \" \" + r\"$duration$\"+ \" \" + r\"$(seconds)$\", fontsize=13)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=13)\n",
    "ax.set_yscale(\"log\")\n",
    "#ax.set_xscale(\"log\")\n",
    "ax.set_xlim([0,0.7])\n",
    "ax.set_ylim([0.5,1*10**4])\n",
    "\n",
    "\n",
    "\n",
    "## FITING\n",
    "x = brevity0.repetitions\n",
    "y = brevity0.duration\n",
    "\n",
    "xplot = np.linspace(0.1, 0.55, 8)\n",
    "popt, pcov = curve_fit(func_log, x, y)\n",
    "ax.plot(xplot, func_exp(xplot, *popt), \"--\", color = \"darkred\", lw = 2)\n",
    "\n",
    "ax.text(0.75, 0.25, r'$Catalan$' + \"\\n\" + r'$\\lambda= $' + \"{:.{}f}\".format(popt[1], 1), horizontalalignment='center', fontsize = 15, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "residuals = y - func_log(x, *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(\"R2: \" + str(r_squared))\n",
    "print()\n",
    "\n",
    "\n",
    "##############################################\n",
    "## 2.frecuencia palabras vs numero de fonemas\n",
    "##############################################\n",
    "def func_exp64(x, a, b):\n",
    "    return a * 35**(-b * x)\n",
    "\n",
    "def func_log64(x, a, b): # FUNCION LOGARITMICA que utiliza los mismos parametros\n",
    "    return 1/(-b) * ( np.log(x)/np.log(35) - np.log(a)/np.log(35) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "brevity1 = lista_words.groupby(\"token\").median()[[\"numphonemes\", \"repetitions\"]]\n",
    "\n",
    "print(\"Frequencia palabras vs numero de fonemas:\")\n",
    "print(scipy.stats.spearmanr(brevity1.numphonemes, np.log(brevity1.repetitions)))\n",
    "\n",
    "\n",
    "# Ploteamos\n",
    "subax = plt.axes([ax.get_position().x1 - .28, ax.get_position().y1 - .28, .28, .28])\n",
    "\n",
    "freq1, pos1 = binear_datos(brevity1.repetitions, brevity1.numphonemes, bins=11, log = True)\n",
    "subax.plot(brevity1.numphonemes, brevity1.repetitions, 'o', lw = 1, ms = 2, zorder=2, alpha=0.6, color = 'lightgray', fillstyle= \"none\")\n",
    "subax.plot(pos1, freq1, 'o', lw = 2, ms = 7, label= 'words', zorder=2, alpha=1, color=\"darkblue\")\n",
    "\n",
    "\n",
    "\n",
    "## FITING#######################################################\n",
    "x = brevity1.repetitions\n",
    "y = brevity1.numphonemes\n",
    "\n",
    "xplot = np.linspace(1.5, 8, 3)\n",
    "popt, pcov = curve_fit(func_log64, x, y)\n",
    "subax.plot(xplot, func_exp64(xplot, *popt), \"--\", color = \"darkred\", lw = 2)\n",
    "subax.text(0.74, 0.73, r'$\\lambda_D = $' + \"{:.{}f}\".format(popt[1], 2), horizontalalignment='center', fontsize = 12, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "residuals = y - func_log64(x, *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(\"R2: \" + str(r_squared))\n",
    "print()\n",
    "\n",
    "\n",
    "subax.set_yscale(\"log\")\n",
    "subax.set_xlabel(r\"$\\widetilde{size}$\" + \" \" +  r\"$(phonemes)$\", fontsize=12)\n",
    "subax.set_xlim([0.5, 7])\n",
    "subax.set_ylim([0.3, 50000])\n",
    "#subax.set_ylabel(\"freq\",rotation=0)\n",
    "\n",
    "\n",
    "##############################################\n",
    "## 3.frecuencia palabras vs numero de characters\n",
    "##############################################\n",
    "def func_exp26(x, a, b):\n",
    "    return a * 26**(-b * x)\n",
    "\n",
    "def func_log26(x, a, b): # FUNCION LOGARITMICA que utiliza los mismos parametros\n",
    "    return 1/(-b) * ( np.log(x)/np.log(26) - np.log(a)/np.log(26) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "brevity2 = lista_words.groupby(\"token\").median()[[\"numletters\", \"repetitions\"]]\n",
    "\n",
    "brevity2=brevity2[brevity2.numletters<15]\n",
    "print(\"Frequencia palabras vs numero de caracteres:\")\n",
    "print(scipy.stats.spearmanr(brevity2.numletters, np.log(brevity2.repetitions)))\n",
    "\n",
    "\n",
    "# Ploteamos\n",
    "subax2 = plt.axes([ax.get_position().x0, ax.get_position().y0, .28, .28])\n",
    "\n",
    "freq2, pos2 = binear_datos(brevity2.repetitions, brevity2.numletters, bins=7, log = True)\n",
    "subax2.plot(brevity2.numletters, brevity2.repetitions, 'o', lw = 1, ms = 2, zorder=2, alpha=0.4, color = 'lightgrey')\n",
    "subax2.plot(pos2, freq2, 'o', lw = 2, ms = 7, label= 'words', zorder=2, alpha=1, color=\"darkblue\")\n",
    "\n",
    "\n",
    "## FITING#######################################################\n",
    "x = brevity2.repetitions\n",
    "y = brevity2.numletters\n",
    "\n",
    "xplot = np.linspace(2, 12, 3)\n",
    "popt, pcov = curve_fit(func_log26, x, y)\n",
    "subax2.plot(xplot, func_exp26(xplot, *popt), \"--\", color = \"darkred\", lw = 2)\n",
    "subax2.text(0.12, 0.12, r'$\\lambda_D = $' + \"{:.{}f}\".format(popt[1], 2), horizontalalignment='center', fontsize = 12, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "residuals = y - func_log26(x, *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(\"R2: \" + str(r_squared))\n",
    "\n",
    "\n",
    "\n",
    "subax2.set_yscale(\"log\")\n",
    "#subax2.set_xscale(\"log\")\n",
    "\n",
    "subax2.set_xlabel(r\"$size$\" + \" \" +  r\"$(chars)$\",rotation=0, fontsize=12)\n",
    "subax2.set_xlim([0.5, 8])\n",
    "subax2.set_ylim([0.2, 10**5])\n",
    "\n",
    "subax2.yaxis.set_label_position(\"right\")\n",
    "subax2.yaxis.set_ticks_position(\"right\")\n",
    "\n",
    "#subax2.set_ylabel(\"freq\",rotation=0)\n",
    "subax2.xaxis.set_label_position(\"top\")\n",
    "subax2.xaxis.set_ticks_position(\"top\")\n",
    "\n",
    "\n",
    "\n",
    "subax2.set_xticks([1,3,5,7])\n",
    "subax2.set_yticks([10**0,10**2, 10**4])\n",
    "\n",
    "#subax2.set_yticks([10**3,10**5])\n",
    "ax.set_xticks([0.1, 0.3, 0.5, 0.7])\n",
    "\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "f.savefig(output_path + \"3_Brevity_words.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Brevity law  Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_datos_brev = tabla_datos.copy()\n",
    "#tabla_datos_brev = tabla_datos_brev.assign(numletterPhon=tabla_datos_brev[tabla_datos_brev.tipe==\"w\"].numletters/tabla_datos_brev[tabla_datos_brev.tipe==\"w\"].numphonemes)\n",
    "#tabla_datos_brev.fillna(method='ffill', inplace=True)\n",
    "\n",
    "lista_fonemabrev = tabla_datos_brev[((tabla_datos_brev.tipe == 'p') & (tabla_datos_brev.token != 'SIL') & (tabla_datos_brev.token != 'VOCNOISE') \n",
    "                            & (tabla_datos_brev.token != 'UNKNOWN') & (tabla_datos_brev.token != 'NOISE') & (tabla_datos_brev.token != 'LAUGH')\n",
    "                            & (tabla_datos_brev.token != 'IVER')\n",
    "                            & (tabla_datos_brev.token.str.endswith(\"}\") == False) & (tabla_datos_brev.token.str.startswith(\"{\") == False))]\n",
    "\n",
    "lista_fonemabrev = lista_fonemabrev.assign(repetitions=lista_fonemabrev.token.map(lista_fonemabrev.token.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brevityp = lista_fonemabrev.groupby(\"token\").median()[[\"duration\", \"repetitions\"]]\n",
    "brevityp = brevityp[brevityp.repetitions>50]\n",
    "brevityp2 = brevityp[brevityp.repetitions>80]\n",
    "\n",
    "print(len(brevityp))\n",
    "#The two-sided p-value for a hypothesis test whose null hypothesis is that two sets of data are uncorrelated, \n",
    "#A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis\n",
    "print(\"Frequencia phonemes vs duracion(segundos:\")\n",
    "print(scipy.stats.spearmanr(brevityp.repetitions, brevityp.duration))\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Frequencia phonemes vs duracion(segundos) si >50:\")\n",
    "print(scipy.stats.spearmanr(brevityp2.repetitions, brevityp2.duration))\n",
    "print()\n",
    "print()\n",
    "\n",
    "### AHORA VAMOS A HACER UN BINING\n",
    "freq, pos = binear_datos(brevityp.repetitions, brevityp.duration, bins=5, log = True)\n",
    "\n",
    "f, ax1 = plt.subplots()\n",
    "ax1.plot(brevityp.duration, brevityp.repetitions, 'o', lw = 1, ms = 5, zorder=2, alpha=0.6, color = 'lightgrey')\n",
    "ax1.plot(pos, freq, 's', lw = 2, ms = 11, label= 'phonemes', zorder=2, alpha=1, color = 'darkorange')\n",
    "\n",
    "ax1.set_xlabel(r\"$Median$\"  + \" \" + r\"$time$\"  + \" \" + r\"$ duration$\"  + \" \" + r\"$ (seconds)$\", fontsize=12)\n",
    "#ax.legend()\n",
    "ax1.set_yscale(\"log\")\n",
    "\n",
    "ax1.set_ylabel(\"Frequency\", fontsize=12)\n",
    "#ax1.legend(frameon=False)\n",
    "\n",
    "ax1.set_xticks([0.05, 0.07, 0.13])\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "def func_exp(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "def func_log(x, a, b): # FUNCION LOGARITMICA que utiliza los mismos parametros\n",
    "    return 1/(-b) * ( np.log(x) - np.log(a) )\n",
    "\n",
    "## FITING\n",
    "x = brevityp2.repetitions\n",
    "y = brevityp2.duration\n",
    "\n",
    "xplot = np.linspace(0.05, 0.07, 3)\n",
    "popt, pcov = curve_fit(func_log, x, y)\n",
    "ax1.plot(xplot, func_exp(xplot, *popt), \"--\", color = \"darkred\", lw = 2)\n",
    "\n",
    "ax1.text(0.8, 0.8, r'$Catalan$' + \"\\n\" + r'$\\lambda = $' + \"{:.{}f}\".format(popt[1], 0), horizontalalignment='center', fontsize = 15, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "residuals = y - func_log(x, *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(\"R2: \" + str(r_squared))\n",
    "print()\n",
    "\n",
    "ax1.set_xlim([0.04, 0.09])\n",
    "\n",
    "ax1.tick_params(axis='x', labelsize=12)\n",
    "ax1.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "f.savefig(output_path + \"3_Brevity_phonemes.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size-rank brevity law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meanDuration = lista_words.groupby(\"token\").duration.median()\n",
    "freq = lista_words.groupby(\"token\").repetitions.mean()\n",
    "df = pd.DataFrame({\"dur\":meanDuration, \"f\":freq })\n",
    "df = df.sort_values(\"f\", ascending=False)\n",
    "df[\"r\"] = np.arange(1,len(df)+1)\n",
    "\n",
    "dfRsup50 = df[df.r>0].copy()\n",
    "\n",
    "df[\"cumdur_rangos\"] = df.dur.cumsum()\n",
    "dfRsup50[\"cumdur_rangos\"] = dfRsup50.dur.cumsum()\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# PLOT ####################################################\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "meanx, meany = binear_datos(dfRsup50.r, dfRsup50.dur, bins=21, log = True)\n",
    "ax.plot(dfRsup50.r, dfRsup50.dur, \"o\", color = \"lightgray\", alpha = 0.8, ms=1, label =r\"$data$\")\n",
    "\n",
    "ax.plot(meanx[0:-1], meany[0:-1], \"o\", color = \"darkblue\", ms=9, alpha = 0.8, label =r\"$binned \\  data$\")\n",
    "\n",
    "\n",
    "\n",
    "# Primera pendiente\n",
    "xplot = dfRsup50.r\n",
    "yplot = 0.059663866*np.log(meanx[2:-1])\n",
    "yplot = yplot - yplot[0] + meany[0]\n",
    "\n",
    "ax.plot(meanx[2:-1], yplot - 0.06, \"--\", color=\"k\", lw=2, label = r\"$\\ell \\sim \\frac{\\alpha}{\\lambda}\\log(r)$\") #label =r\"$\\hat{\\ell}_1 = \\frac{\\alpha _1}{\\lambda} \\cdot \\log(r _i) + K_1$\")\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "ax.set_xlabel(\"Rank r \", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\hat{\\ell} $\", fontsize=14)\n",
    "\n",
    "ax.set_yticks([0.2, 0.5, 0.8])\n",
    "\n",
    "ax.set_xlim([0.8, 6000])\n",
    "\n",
    "ax.set_ylim([0.01, 0.8])\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "ax.legend(frameon=False, fontsize=12,title=r\"$Catalan$\", title_fontsize=12)\n",
    "f.savefig(output_path + \"SI_RANK_DURATION.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Menzerath Altmann law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Existen divesas posibles definiciones de Menzerath Altmann law. Por ello vamos a intentar varias y comaprarlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Sentences length (number of words) VS words length (number of letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceSize_numwords = []\n",
    "WordsSize_numletters = []\n",
    "WordsSize_numphonemes = []  # Para la definicion2\n",
    "WordsSize_seconds = []  # Para la definicion3\n",
    "\n",
    "\n",
    "\n",
    "for sentence in tabla_datos.sentence.unique():\n",
    "    tablaSentence = tabla_datos[tabla_datos.sentence == sentence]\n",
    "    numwords = len(tablaSentence[tablaSentence.tipe == \"w\"])\n",
    "    if numwords == 100:\n",
    "        stop\n",
    "    SentenceSize_numwords.append(numwords)\n",
    "\n",
    "    numletters_mean = np.mean(tablaSentence[tablaSentence.tipe == \"w\"].numletters)\n",
    "    WordsSize_numletters.append(numletters_mean)\n",
    "    \n",
    "    # Para la definicion2\n",
    "    numphonemes_mean = np.mean(tablaSentence[tablaSentence.tipe == \"w\"].numphonemes)\n",
    "    WordsSize_numphonemes.append(numphonemes_mean)\n",
    "    \n",
    "    # Para la definicion3\n",
    "    word_duration_mean = np.mean(tablaSentence[tablaSentence.tipe == \"w\"].duration)\n",
    "    WordsSize_seconds.append(word_duration_mean)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "def func_powerlawexp(x, alfa, beta, c):\n",
    "    return alfa * x**(beta) * np.exp(-c*x)\n",
    "\n",
    "# def func_powerlawexp(x, beta):\n",
    "#     #c = b/6\n",
    "#     #0.4 = a*e^(-b/6)\n",
    "    \n",
    "#     return (0.4/(np.exp(-beta/6)) * x**(-beta) * np.exp(-beta/6*x))\n",
    "\n",
    "numbins=10\n",
    "\n",
    "for setsize in [41]: # [41, 80]\n",
    "    for bin15 in [False]:\n",
    "    \n",
    "        # FIGURA MENZERATH\n",
    "        f, ax = plt.subplots()\n",
    "\n",
    "        ###########################################################################\n",
    "        # MENZERATH FRSES WORDS (SECONDS) #########################################\n",
    "        #########################################################################################################\n",
    "\n",
    "        Mentable_sec = pd.DataFrame({\"SentSize\":SentenceSize_numwords, \"WordsSize\": WordsSize_seconds})\n",
    "        Mentable_sec1 = Mentable_sec[ Mentable_sec[\"SentSize\"]<setsize]\n",
    "\n",
    "        azul = Mentable_sec1.groupby(\"SentSize\").mean()\n",
    "        pos0 = azul.index.values\n",
    "        freq0 = azul.WordsSize.values\n",
    "        pos015, freq015 = binear_datos(pos0, freq0, bins=numbins, log = False)\n",
    "\n",
    "\n",
    "        # MAIN PLOT\n",
    "        #ax.plot(Mentable_sec.SentSize.values, Mentable_sec.WordsSize.values , 'o', lw = 2, ms = 2, zorder=2, alpha=1, color = \"gray\")\n",
    "        ax.plot(Mentable_sec1.SentSize.values[0:3000], Mentable_sec1.WordsSize.values[0:3000], 'o', ms = 1, zorder=2, alpha=0.5, color = 'lightgrey', label = r\"$data$\")\n",
    "        if bin15 == False:\n",
    "            ax.plot(pos0, freq0, 'o', lw = 2, ms = 8, zorder=2, alpha=1, color = \"darkblue\", label = r\"$mean$\")\n",
    "        elif bin15 == True:\n",
    "            ax.plot(pos015, freq015, 'o', lw = 2, ms = 8, zorder=2, alpha=1, color = \"darkblue\", label = r\"$mean$\")\n",
    "\n",
    "        ax.set_xlabel(\"BG size in number of words\")\n",
    "        ax.set_ylabel(r\"$Word$\" + \" \" + r\"$\\overline{size}$\" + \" \" + r\"$(seconds)$\")\n",
    "\n",
    "        ax.set_ylim([0.0, 0.45])\n",
    "\n",
    "        ax.set_xticks([0, 15, 30, 45])\n",
    "        ax.set_yticks([0, 0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "\n",
    "        x = np.asarray(pos0)\n",
    "        y = np.asarray(freq0)\n",
    "\n",
    "\n",
    "        # POWER LAW EXPONENTIAL\n",
    "        print(\"Power law exponential seconds:\")\n",
    "        #popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "        popt, pcov = curve_fit(func_powerlawexp, pos0, freq0)\n",
    "\n",
    "        expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        ax.plot(x, expy, \"--\", lw=2, color=\"red\", label = r\"$fit$\" + \" \" + r\"$to$\" + \" \" + r\"$y = ax^b e^{-cx}$\")\n",
    "\n",
    "        velocityx = x\n",
    "        velocityy = 1/expy\n",
    "\n",
    "        print(\"exponentes = \" + str(popt))\n",
    "\n",
    "        # GET R2\n",
    "        residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        # residuals = y - func_powerlawexp(x, popt[0])\n",
    "\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        R2 = 1 - (ss_res / ss_tot)\n",
    "        print(\"R2:\" + str(R2))\n",
    "        print(\"\")\n",
    "\n",
    "        ax.legend(frameon = False, loc = 4, fontsize=11,title=r\"$Catalan$\", title_fontsize=12)\n",
    "\n",
    "        #ax.set_xlim([0, 40])\n",
    "\n",
    "\n",
    "        ax.tick_params(axis='x', labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        # NUM LETTERS ############################################################################\n",
    "        Mentableletters = pd.DataFrame({\"SentSize\":SentenceSize_numwords, \"WordsSize\": WordsSize_numletters})\n",
    "        Mentableletters1 = Mentableletters[ Mentableletters[\"SentSize\"]<setsize]\n",
    "        #Menzerathletters = Mentableletters.groupby(\"SentSize\").mean()\n",
    "\n",
    "        subax = plt.axes([ax.get_position().x0, ax.get_position().y0, .25, .25])\n",
    "\n",
    "        #pos1, freq1 = binear_datos(Mentableletters1.SentSize.values, Mentableletters1.WordsSize.values, bins=15, log = False)\n",
    "        azul = Mentableletters1.groupby(\"SentSize\").mean()\n",
    "        pos1 = azul.index.values\n",
    "        freq1 = azul.WordsSize.values\n",
    "        \n",
    "        pos115, freq115 = binear_datos(pos1, freq1, bins=numbins, log = False)\n",
    "\n",
    "\n",
    "\n",
    "        # MAIN PLOT\n",
    "        #subax.plot(Mentableletters.SentSize.values, Mentableletters.WordsSize.values , 'o', lw = 2, ms = 2, zorder=2, alpha=1, color = \"gray\")\n",
    "        #subax.plot(Menzerathletters, 'o', lw = 2, ms = 2, label= 'seconds', zorder=2, alpha=1, color = \"gray\")\n",
    "        subax.plot(Mentableletters.SentSize.values[0:3000], Mentableletters.WordsSize.values[0:3000], 'o', ms = 1, zorder=2, alpha=0.5, color = 'lightgrey')\n",
    "\n",
    "        \n",
    "        if bin15 == False:\n",
    "            subax.plot(pos1, freq1, 'o', lw = 2, ms = 5, zorder=2, alpha=1, color = \"darkblue\")\n",
    "        elif bin15 == True:\n",
    "            subax.plot(pos115, freq115, 'o', lw = 2, ms = 5, zorder=2, alpha=1, color = \"darkblue\")\n",
    "        \n",
    "        \n",
    "\n",
    "        #subax.set_xlabel(\"BG (words)\")\n",
    "        subax.set_ylabel( r\"$Word$\" + \" \" + r\"$\\overline{size}$\" + \"\\n\" + r\"$(chars)$\", rotation = 0)\n",
    "        subax.yaxis.set_label_coords(1.22, 0.65)\n",
    "\n",
    "\n",
    "        subax.yaxis.set_label_position(\"right\")\n",
    "        subax.yaxis.set_ticks_position(\"right\")\n",
    "\n",
    "        #subax2.set_ylabel(\"freq\",rotation=0)\n",
    "        subax.xaxis.set_label_position(\"top\")\n",
    "        subax.xaxis.set_ticks_position(\"top\")\n",
    "        subax.set_ylim([3.5, 4.1])\n",
    "\n",
    "        subax.set_yticks([3.6, 4])\n",
    "        subax.set_xticks([10, 30, 50])\n",
    "        #subax.set_xlim([0, 40])\n",
    "\n",
    "\n",
    "        x = np.asarray(pos1)[2::]\n",
    "        y = np.asarray(freq1)[2::]\n",
    "\n",
    "        subax.tick_params(axis='x', labelsize=12)\n",
    "        subax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        # POWER LAW EXPONENTIAL\n",
    "        print(\"Power law exponential chars:\")\n",
    "        popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "        expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        subax.plot(x, expy, \"--\", lw=2, color=\"red\")\n",
    "        print(\"exponentes = \" + str(popt))\n",
    "\n",
    "        # GET R2\n",
    "        residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        R2 = 1 - (ss_res / ss_tot)\n",
    "        print(\"R2:\" + str(R2))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        #################################################################################\n",
    "        # Sentences length (number of words) VS words length (number of phonemes)\n",
    "        #############################################################################\n",
    "\n",
    "        # Ploteamos\n",
    "        subax2 = plt.axes([ax.get_position().x1 - .25, ax.get_position().y1 - .25, .25, .25])\n",
    "\n",
    "        Mentable = pd.DataFrame({\"SentSize\":SentenceSize_numwords, \"WordsSize\": WordsSize_numphonemes})\n",
    "        Mentable1 = Mentable[Mentable.SentSize<setsize]\n",
    "        #Menzerathphonemes = Mentable.groupby(\"SentSize\").mean()\n",
    "\n",
    "\n",
    "        #pos2, freq2 = binear_datos(Mentable1.SentSize.values, Mentable1.WordsSize.values, bins=15, log = False)\n",
    "        azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "        pos2 = azul.index.values\n",
    "        freq2 = azul.WordsSize.values\n",
    "        pos215, freq215 = binear_datos(pos2, freq2, bins=numbins, log = False)\n",
    "\n",
    "        # MAIN PLOT\n",
    "        #subax2.plot(Menzerath, 'o', lw = 2, ms = 2, zorder=2, alpha=1, color = \"gray\")\n",
    "        subax2.plot(Mentable.SentSize.values[0:3000], Mentable.WordsSize.values[0:3000], 'o', ms = 1, zorder=2, alpha=0.5, color = 'lightgrey')\n",
    "        if bin15 == False:\n",
    "            subax2.plot(pos2, freq2, 'o', lw = 2, ms = 5, zorder=2, alpha=1, color = \"darkblue\")\n",
    "        elif bin15 == True:\n",
    "            subax2.plot(pos215, freq215, 'o', lw = 2, ms = 5, zorder=2, alpha=1, color = \"darkblue\")\n",
    "        \n",
    "        \n",
    "        #subax2.set_xlabel(r\"$BG (words)$\")\n",
    "        subax2.set_ylabel(r\"$Word$\" + \" \" + r\"$\\overline{size}$\" + \"\\n\" + r\"$(phon)$\", rotation = 0)\n",
    "        subax2.yaxis.set_label_coords(-0.25, 0.35)\n",
    "\n",
    "        subax2.set_xticks([10, 30, 50])\n",
    "        subax2.set_yticks([3.4, 3.8])\n",
    "        subax2.set_ylim([3.3,3.9])\n",
    "        #subax2.set_xlim([0, 40])\n",
    "        subax2.tick_params(axis='x', labelsize=12)\n",
    "        subax2.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        x = np.asarray(pos2)[2::]\n",
    "        y = np.asarray(freq2)[2::]\n",
    "\n",
    "        # POWER LAW EXPONENTIAL\n",
    "        print(\"Power law exponential phonemes:\")\n",
    "        popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "        expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        subax2.plot(x, expy, \"--\", lw=2, color=\"red\")\n",
    "        print(\"exponentes = \" + str(popt))\n",
    "\n",
    "        # GET R2\n",
    "        residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        R2 = 1 - (ss_res / ss_tot)\n",
    "        print(\"R2:\" + str(R2))\n",
    "        print(\"\")\n",
    "\n",
    "        if setsize == 80:\n",
    "            if bin15 == False:\n",
    "                f.savefig(output_path + \"5_Menzerath_BG_WORDS_allBG.pdf\")\n",
    "            elif bin15 == True:\n",
    "                f.savefig(output_path + \"5_Menzerath_BG_WORDS_allBG_bin15.pdf\")\n",
    "        elif setsize == 41:\n",
    "            subax.set_xticks([10, 30])\n",
    "            subax2.set_xticks([10, 30])\n",
    "            subax2.set_xlim([0, 42])\n",
    "            subax.set_xlim([0, 42])\n",
    "            \n",
    "            if bin15 == False:\n",
    "                f.savefig(output_path + \"5_Menzerath_BG_WORDS_less41.pdf\")\n",
    "            elif bin15 == True:\n",
    "                f.savefig(output_path + \"5_Menzerath_BG_WORDS_less41_bin15.pdf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todos los posibles ajustes de MAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def func_powerlawexp(x, alfa, beta, c):\n",
    "    return alfa * x**(beta) * np.exp(-c*x)\n",
    "\n",
    "\n",
    "def adjust_powerlawexp(xdata, ydata):\n",
    "    popt, pcov = curve_fit(func_powerlawexp, xdata, ydata)\n",
    "    expy = func_powerlawexp(xdata, popt[0], popt[1], popt[2])\n",
    "\n",
    "    # GET R2\n",
    "    residuals = ydata - func_powerlawexp(xdata, popt[0], popt[1], popt[2])\n",
    "    # residuals = y - func_powerlawexp(x, popt[0])\n",
    "\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((ydata - np.mean(ydata))**2)\n",
    "    R2 = 1 - (ss_res / ss_tot)\n",
    "    print(\"a|b|c| b/c = \" + str(popt[0].round(2)) + \" | \" + str(popt[1].round(4)) +\" | \"+  str(popt[2].round(5))+\" | \"+  str((popt[1]/popt[2]).round()) + \"  R2: \" + str(R2.round(2)))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# ALL DATA BIN AZUL\n",
    "print(\"###### ALL DATA AZUL ############################\")\n",
    "azul = Mentable_sec.groupby(\"SentSize\").mean()\n",
    "print(\"BG seconds  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "Mentable1 = Mentable[Mentable.SentSize>2]\n",
    "azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "print(\"BG>2 phonemes  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "azul = Mentable.groupby(\"SentSize\").mean()\n",
    "print(\"BG phonemes  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "Mentable1 = Mentableletters[Mentableletters.SentSize>2]\n",
    "azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "print(\"BG>2 letters  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "azul = Mentableletters.groupby(\"SentSize\").mean()\n",
    "print(\"BG letters  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ALL DATA BIN AZUL\n",
    "print(\"###### DATA AZUL < 40 ############################\")\n",
    "\n",
    "Mentable_sec1 = Mentable_sec[Mentable_sec.SentSize<40]\n",
    "azul = Mentable_sec1.groupby(\"SentSize\").mean()\n",
    "print(\"BG seconds  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "Mentable1 = Mentable[Mentable.SentSize<40]\n",
    "Mentable1 = Mentable1[Mentable1.SentSize>2]\n",
    "azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "print(\"BG>2 phonees  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "Mentable1 = Mentable[Mentable.SentSize<40]\n",
    "azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "print(\"BG phonees  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "\n",
    "Mentableletters1 = Mentableletters[Mentableletters.SentSize<40]\n",
    "Mentableletters1 = Mentableletters1[Mentableletters1.SentSize>2]\n",
    "azul = Mentableletters1.groupby(\"SentSize\").mean()\n",
    "print(\"BG>2 letters  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "Mentableletters1 = Mentableletters[Mentableletters.SentSize<40]\n",
    "azul = Mentableletters1.groupby(\"SentSize\").mean()\n",
    "print(\"BG letters  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "# ALL DATA GRIS\n",
    "print(\"###### ALL DATA GRIS ############################\")\n",
    "print(\"BG seconds  \")\n",
    "adjust_powerlawexp(Mentable_sec.SentSize.values, Mentable_sec.WordsSize.values)\n",
    "\n",
    "print(\"BG >2 phonemes  \")\n",
    "Mentable1 = Mentable[Mentable.SentSize>2]\n",
    "adjust_powerlawexp(Mentable1.SentSize.values, Mentable1.WordsSize.values)\n",
    "\n",
    "print(\"BG phonemes  \")\n",
    "adjust_powerlawexp(Mentable.SentSize.values, Mentable.WordsSize.values)\n",
    "\n",
    "print(\"BG letters >2 \")\n",
    "Mentable1 = Mentableletters[Mentableletters.SentSize>2]\n",
    "adjust_powerlawexp(Mentable1.SentSize.values, Mentable1.WordsSize.values)\n",
    "\n",
    "print(\"BG letters  \")\n",
    "adjust_powerlawexp(Mentableletters.SentSize.values, Mentableletters.WordsSize.values)\n",
    "\n",
    "\n",
    "# DATA < 40 GRIS\n",
    "print(\"###### DATA GRIS < 40 ############################\")\n",
    "print(\"BG seconds  \")\n",
    "Mentable_sec1 = Mentable_sec[Mentable_sec.SentSize<40]\n",
    "adjust_powerlawexp(Mentable_sec1.SentSize.values, Mentable_sec1.WordsSize.values)\n",
    "\n",
    "print(\"BG>2 phonemes  \")\n",
    "Mentable1 = Mentable[Mentable.SentSize<40]\n",
    "Mentable1 = Mentable1[Mentable1.SentSize>2]\n",
    "adjust_powerlawexp(Mentable1.SentSize.values, Mentable1.WordsSize.values)\n",
    "\n",
    "print(\"BG phonemes  \")\n",
    "Mentable1 = Mentable[Mentable.SentSize<40]\n",
    "adjust_powerlawexp(Mentable1.SentSize.values, Mentable1.WordsSize.values)\n",
    "\n",
    "print(\"BG >2 letters  \")\n",
    "Mentableletters1 = Mentableletters[Mentableletters.SentSize<40]\n",
    "Mentableletters1 = Mentableletters1[Mentableletters1.SentSize>2]\n",
    "adjust_powerlawexp(Mentableletters1.SentSize.values, Mentableletters1.WordsSize.values)\n",
    "\n",
    "\n",
    "print(\"BG letters  \")\n",
    "Mentableletters1 = Mentableletters[Mentableletters.SentSize<40]\n",
    "adjust_powerlawexp(Mentableletters1.SentSize.values, Mentableletters1.WordsSize.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Words length (number of phonemes) VS phonemes length (seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordLength_num_phonemes = tabla_datos[tabla_datos.tipe == \"w\"].numphonemes\n",
    "WordLength_num_phonemes = tabla_datos[tabla_datos.tipe == \"p\"].groupby(\"numtoken\").count()[\"token\"]\n",
    "PhonemeLength_seconds = tabla_datos[tabla_datos.tipe == \"p\"].groupby(\"numtoken\").mean().duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_powerlawexp(x, alfa, beta, gamma):\n",
    "    return alfa * x**(beta) * np.exp(-gamma*x)\n",
    "\n",
    "Mentable = pd.DataFrame({\"WordSize\":WordLength_num_phonemes.values, \"PhoneSize\": PhonemeLength_seconds.values})\n",
    "Menzerath = Mentable.groupby(\"WordSize\").mean()\n",
    "Menzerath = Menzerath[0:17]\n",
    "\n",
    "\n",
    "# FIGURA MENZERATH\n",
    "f, ax = plt.subplots()\n",
    "pos0, freq0 = binear_datos(Mentable.WordSize.values, Mentable.PhoneSize.values, bins=12, log = False)\n",
    "\n",
    "# MAIN PLOT\n",
    "ax.plot(Mentable.WordSize.values[0:1000], Mentable.PhoneSize.values[0:1000], 'o', ms = 1, zorder=2, alpha=0.5, label = r\"$data$\", color = 'lightgrey')\n",
    "ax.plot(Menzerath, 's', lw = 2, ms = 11, zorder=2, alpha=0.8, color=\"darkorange\", label=r\"$mean$\")\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Word length(<phonem>)\",fontsize=12)\n",
    "ax.set_ylabel(\"Phoneme length (<sec>)\", fontsize=12)\n",
    "\n",
    "#ax.set_xscale(\"log\")\n",
    "#ax.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "ax.set_xticks([2, 6, 10, 14, 18])\n",
    "ax.set_yticks([0.05, 0.08, 0.11])\n",
    "ax.set_ylim([0.03,0.13])\n",
    "ax.set_xlim([1, 19])\n",
    "\n",
    "x = pos0\n",
    "y = freq0\n",
    "\n",
    "# POWER LAW EXPONENTIAL\n",
    "print(\"Power law exponential:\")\n",
    "popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "ax.plot(x, expy, \"--\", lw=2, color=\"red\", label = r\"$fit$\" + \" \" + r\"$to$\" + \" \" + r\"$y = ax^b e^{-cx}$\")\n",
    "print(\"exponentes = \" + str(popt))\n",
    "\n",
    "# GET R2\n",
    "residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y - np.mean(y))**2)\n",
    "R2 = 1 - (ss_res / ss_tot)\n",
    "print(\"R2:\" + str(R2))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "ax.legend(frameon = False, loc = \"best\", fontsize=12,title=r\"$Catalan$\", title_fontsize=12)\n",
    "\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "\n",
    "ax.set_xlabel(r\"$Word$\" + \" \" + r\"$\\overline{size}$\" + \" \" + r\"$in$\" + \" \" + r\"$number$\" + \" \" + r\"$of$\" + \" \" + r\"$phonemes$\")\n",
    "ax.set_ylabel(r\"$Phoneme$\" + \" \" + r\"$\\overline{size}$\" + \" \" + r\"$(seconds)$\")\n",
    "\n",
    "\n",
    "f.savefig(output_path + \"5_Menzerath_Words_Phonemes.pdf\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
